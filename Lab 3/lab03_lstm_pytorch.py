# -*- coding: utf-8 -*-
"""Lab03_LSTM_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XtMwhZE43As4OCtegR10HnktE4Ovqiur

## Part 1: LSTM for Sequence Classification on the IMDB dataset

In a first exercise we want to build a model that can classify movie reviews as positive or negative based on their sentiment. To do so we train a model using the IMDB dataset.

The IMDB dataset consists of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer "3" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: "only consider the top 10,000 most common words, but eliminate the top 20 most common words".
"""

# # Install packages:
# pip install torchdata
# pip install torchtext

# Additionally you may need:
# pip install portalocker

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import numpy
import pandas as pd
# fix random seed so every student get the same results
numpy.random.seed(7)

import torch
from torch import nn
from torch.nn import functional as F
from torchtext.datasets import imdb
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import Vocab

# %matplotlib inline
from matplotlib import pyplot as plt

"""We need to load the IMDB dataset. We are constraining the dataset to the top 5,000 words. We also split the dataset into train (50%) and test (50%) sets."""

class VocabOneHot(): # Helper class for processing the
    def __init__(self,
                 tokenizer,
                 num_words = 5000):
        self.counts = {} # Dictionary for storing the counts of each word
        self.tokenizer = tokenizer
        self.num_words = num_words # maximal number of words to use
        self.fitted = False
        self.lookup = {} # lookup table assigning to each word an integer

    def fit(self, x_iter):
        self.counts = {} # Reinitialize the counts
        for label, line in train_iter: #iterate over the instances
            tokens = self.tokenizer(line)
            for t in tokens:
                if t in self.counts:
                    self.counts[t] += 1
                else:
                    self.counts[t] = 1
        count_series = pd.Series(self.counts).sort_values(ascending=False) # create Series for easy manipulation
        self.lookup = {word[0]:i+1 if i<self.num_words-2 else self.num_words-1
                       for i, word in enumerate(count_series.reset_index().to_numpy())}
        self.fitted = True

    def transform(self, tokenized_sentence, length = 100):
        assert self.fitted, "The instance has not been fitted yet"
        non_padded= [self.lookup[word]
                if word in self.lookup.keys()
                else self.num_words-1
                    for word in tokenized_sentence]
        non_padded = np.array(non_padded) # create unpadded version
        if len(non_padded) == length: # leave the sequence as it stands
            return non_padded
        elif len(non_padded) > length: # truncate
            return non_padded[:length]
        else: # pad
            padding = np.zeros(length - len(non_padded))
            padded = np.hstack([padding, non_padded]).astype(int)
            return padded

train_iter = imdb.IMDB("data/",  split="train")
test_iter = imdb.IMDB("data/", split="test")
tokenizer = get_tokenizer("basic_english", language='en')

# Could be necessary depending of the version of torchtext
# train_iter = list(train_iter)
# test_iter = list(test_iter)

vocab = VocabOneHot(tokenizer) # Initialize the vocabulary

sr = vocab.fit(train_iter) # Fit the vocabulary to the training set

X_train = []
y_train = []
ys = []
for i, (label, line) in enumerate(train_iter): # Create training data
    X_train += [vocab.transform(tokenizer(line))]
    y_train += [label]
X_train = np.array(X_train).astype(int)
y_train = np.array(y_train)
X_test = []
y_test = []
for i, (label, line) in enumerate(test_iter): # Create test data
    X_test += [vocab.transform(tokenizer(line))]
    y_test += [label]
X_test = np.array(X_test).astype(int)
y_test = np.array(y_test)
y_test[y_test==1] = 0
y_test[y_test==2] = 1
y_train[y_train==1] = 0
y_train[y_train==2] = 1

"""<b>Exercise 1</b>:

Create the training data by selecting balanced subsets of the training and test splits, each of size 5000.
"""

n_pos_samples = 0 # modify
n_neg_samples = 0  # modify
positives_train = X_train[y_train == 0]
negatives_train = X_train[y_train == 1]
positives_test = X_test[y_test == 0]
negatives_test = X_test[y_test == 1]

# YOUR CODE GOES HERE

try:
    assert X_train.shape == (5000, 100)
    assert X_test.shape == (5000, 100)
    print("Testing successful.")
except:
    print("Tests failed.")

"""We can now define and fit our LSTM model.

The first layer is the embedded layer that uses 32 length vectors to represent
 each word. The next layer is the LSTM layer with 5 memory units (smart neurons).
 Finally, because this is a classification problem we use a dense output
 layer with a single neuron and a sigmoid activation function to make 0 or 1
 predictions for the two classes (good or bad) in the problem.

Because it is a binary classification problem, log loss is used as the loss
function. The efficient ADAM optimization
algorithm is used to do the gradient descent.

<b>Exercise 2</b>:

Explore the class LSTM_model, try to understand the details of the forward method, and comment the code
"""

# Create dataloaders containing the test and training data
data_train = torch.utils.data.TensorDataset(torch.Tensor(X_train).long(),
                               torch.Tensor(y_train).long())
data_test = torch.utils.data.TensorDataset(torch.Tensor(X_test).long(),
                               torch.Tensor(y_test).long())
train_dl = torch.utils.data.DataLoader(data_train, batch_size = 256, shuffle=True)
test_dl = torch.utils.data.DataLoader(data_test, batch_size = 256)

class RecurrentDropout(nn.Module):
    """
    Module for Dropout in recurrent architectures.
    The mask is initialized on the first iteration, and used along the sequence.
    """
    def __init__(self, p = 0.1):
        super().__init__()
        self.id = nn.Identity()
        self.bern = torch.distributions.Bernoulli(p)
        self.p = p
    def forward(self, x, first=True):
        if self.training:
            if first:
                self.mask = self.bern.sample([x.shape[0], x.shape[1]])
            else:
                pass
            x = self.id(x)
            x[self.mask.bool()] = 0
            return (x / (1 - self.p))
        else:
            return self.id(x)

class LSTM_model(nn.Module):
    def __init__(self,
                num_embeddings = 5000,
                embedding_dim = 32,
                lstm_dim = 10,
                target_size = 1,
                recurrent_dropout = 0,
                seq_dropout = 0):
        super().__init__()
        self.lstm_dim = lstm_dim
        self.embedding = nn.Embedding(num_embeddings,
                                     embedding_dim)
        self.lstm = nn.LSTMCell(embedding_dim, lstm_dim)
        self.clf = nn.Linear(lstm_dim, target_size)
        self.recurrent_do = RecurrentDropout(p = recurrent_dropout)
        self.seq_dropout = nn.Dropout1d(p=seq_dropout)

    def forward(self, x, autoregressive_iterations=None):
        x = self.embedding(x)
        x = self.seq_dropout(x)
        for i in range(x.shape[-2]):
            if i == 0:
                hn, cn = self.lstm(x[:, i])
                hn = self.recurrent_do(hn, first=True)
            else:
                hn, cn = self.lstm(x[:, i], (hn,
                                             cn))
                hn = self.recurrent_do(hn, first=False)
        if autoregressive_iterations is None:
            return self.clf(hn)
        else:
            output = []
            for it in range(autoregressive_iterations):
                token = self.clf(hn).argmax(axis=1)
                output += token
                embed_token = self.embedding(token)
                hn, cn = self.lstm(embed_token, (hn, cn))
        return torch.stack(output)

"""Now the boilerplate code used for training and evaluating the model will be defined"""

def train_batch(model, optimizer, loss_fn, dataloader):
    losses = []
    hit = 0
    miss = 0
    model.train()
    for batch in dataloader:
        batch[0] = batch[0].to(device)
        batch[1] = batch[1].to(device)
        output = model(batch[0])
        loss = loss_fn(output.squeeze(), batch[1].squeeze().float())
        losses += [loss.item()]
        loss.backward()
        pred =  (output > 0).long()
        equal = pred.squeeze() == batch[1].squeeze()
        hit += equal.sum()
        miss += (~equal).sum()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
        optimizer.step()
        optimizer.zero_grad()
    return np.mean(losses), hit/(hit + miss)

def test_batch(model, optimizer, loss_fn, dataloader):
    losses = []
    hit = 0
    miss = 0
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            batch[0] = batch[0].to(device)
            batch[1] = batch[1].to(device)
            output = model(batch[0])
            loss = loss_fn(output.squeeze(), batch[1].squeeze().float())
            losses += [loss.item()]
            pred =  (output > 0).long()
            equal = pred.squeeze() == batch[1].squeeze()
            hit += equal.sum()
            miss += (~equal).sum()
    return np.mean(losses), hit/(hit + miss)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = LSTM_model()
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), 0.001)
loss_fn = nn.BCEWithLogitsLoss()

"""<b>Exercise 3:</b>

Train a model for only 10 epochs and evaluate the model on the test data. Use a batch size of 64 reviews to update the weights in the LSTM. Use X_test and y_test to validate the model.
"""

for epoch in range(10):
    train_loss =  [0, 0] # modify
    test_loss = [0, 0] # modify
    print(f"""Epoch: {epoch},
    train_loss: {train_loss[0]},
    train_acc: {train_loss[1]},
    test_loss: {test_loss[0]},
    test_acc: {test_loss[1]}""")

"""<b>Exercise 4:</b>

Once we fitted the model, we estimate the performance of the model on unseen reviews.


Modify the test_batch function to create predictions for the test set (note that the model outputs logits and no probabilities, hint: torch.sigmoid).

Predict the instances found in the test dataloader, use a ROC curve to show the performance of the model.

What conclusions can you draw from the ROC curve?
"""

def predict(model, optimizer, loss_fn, dataloader):
    # YOUR CODE GOES HERE
    return None

preds = predict(model, optimizer, loss_fn, test_dl)

# YOUR CODE GOES HERE

"""## Part 2: Generate sentences using LSTMs

In this part of the lab we are using a pretrained model (you can train the model on your own, but this will take some time). This model was trained to predict the next character given a sequence of 100 previous characters. This model can be used to generate new sentences/phrases. Your task is to use this model and generate sentences with it. The model was pretrained on a book called "wonderland".

First of all we want to load the data:
"""

# load ascii text and covert to lowercase
filename = "wonderland.txt"
raw_text = open(filename,encoding="utf8").read()
raw_text = raw_text.lower()
# if there is an error use (python 2 vs. python 3):
#raw_text = open(filename).read()
#raw_text = raw_text.lower().decode('utf-8')

"""Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.

We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer.
"""

# create mapping of unique chars to integers
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))

"""For example, the list of unique unicode sorted lowercase characters in the book is as follows:"""

print(chars)

"""You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process. In this lab we skip this process.

Now that the book has been loaded and the mapping prepared, we can summarize the dataset.
"""

n_chars = len(raw_text)
n_vocab = len(chars)
print("Total Characters: " + str(n_chars))
print("Total Vocab: " + str(n_vocab))

"""We can see that the book has just over 160,000 characters and that when
converted to lowercase that there are only 61 distinct characters in the vocabulary for the network to learn.

We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.

In this tutorial (as explained above) we will split the book text up into subsequences with a fixed length of 100 characters.

Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it.

For example, if the sequence length is 5 (for simplicity) then the first two training patterns would be as follows:

CHAPT -> E, HAPTE -> R

As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier.

<b>Exercise 5:</b>

Create all patterns.

Create a list of sequences dataX that contains all windows of the book and a list of following characters dataY.
So each entry in the list dataX contains of a vector of 100 integer values representing the characters that occur in the window.
Each entry in the list dataY contains the following character for the associated window in dataX.

Hint: Use the dictionary char_to_int to map the characters to integers. You can find an example entry of dataX and dataY in the test cell below.
"""

# prepare the dataset of input to output pairs encoded as integers
seq_length = 100
dataX = []
dataY = []

#YOUR CODE GOES HERE

n_patterns =
print("Total Patterns: " +  str(n_patterns))

"""Check if everything is correct:"""

try:
    assert n_patterns >= 163717
    assert dataX[0] == [60, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34, 57, 48, 1, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 1, 38, 43, 1, 52, 44, 43, 33, 34, 47, 41, 30, 43, 33, 9, 1, 31, 54, 1, 41, 34, 52, 38, 48, 1, 32, 30, 47, 47, 44, 41, 41, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44]
    assert dataY[0] == 35
    print("Testing successful.")
except:
    print("Tests failed.")

"""Running the code to this point shows us that when we split up the dataset into training data for the network to learn that we have just under 160,000 training patterns.

Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.

First we must transform the list of input sequences into the form [samples, features] expected by an LSTM network (if we would have more features per time step the dimension would be [samples, time_steps, features]).

Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 61 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 61, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.
"""

# reshape X to be [samples, time steps]
X = numpy.reshape(dataX, (n_patterns, seq_length))
# one hot encode the output variable
Y_onehot = np.eye(len(chars))[dataY]

"""Now we can define our model. It consists of an embedding layer that embeds each character in a 32-dimensional feature space. The next layer is a layer with 256 LSTM units followed by a Dropout-Layer to reduce overfitting. The last layer is a Dense-Layer used to predict the probabilities for each of the 61 characters."""

embedding_vector_length = 32
top_words = n_vocab
model = LSTM_model(num_embeddings = 61,
                 embedding_dim = 32,
                lstm_dim = 256,
                target_size = 61,
                recurrent_dropout=0.15,
                seq_dropout=0.1)
model.to(device)

"""We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns."""

def train_batch(model, optimizer, loss_fn, dataloader):
    losses = []
    hit = 0
    miss = 0
    model.train()
    for batch in dataloader:
        batch[0] = batch[0].to(device)
        batch[1] = batch[1].to(device)
        output = model(batch[0])
        loss = loss_fn(output.squeeze(), batch[1].squeeze().float())

        losses += [loss.item()]
        loss.backward()
        pred =  output.argmax(axis=1)
        equal = pred.squeeze() == batch[1].argmax(axis=1).squeeze()
        hit += equal.sum()
        miss += (~equal).sum()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)
        optimizer.step()
        optimizer.zero_grad()
    return np.mean(losses), hit/(hit + miss)
def test_batch(model, optimizer, loss_fn, dataloader):
    losses = []
    hit = 0
    miss = 0
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            batch[0] = batch[0].to(device)
            batch[1] = batch[1].to(device)
            output = model(batch[0])
            loss = loss_fn(output.squeeze(), batch[1].argmax(axis=1).squeeze().float())
            losses += [loss.item()]
            pred =  output.argmax(axis=1)
            equal = pred.squeeze() == batch[1].squeeze()
            hit += equal.sum()
            miss += (~equal).sum()
    return np.mean(losses), hit/(hit + miss)

data = torch.utils.data.TensorDataset(torch.Tensor(dataX).long(),
                                        torch.Tensor(Y_onehot).long())
dataloader = torch.utils.data.DataLoader(data, batch_size = 256, shuffle=True)
optimizer = torch.optim.Adam(model.parameters(), 0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)
loss_fn = nn.CrossEntropyLoss()

"""Training this model really takes some time. So you can skip the learning step and use the pretrained model instead."""

for epoch in range(20):
    train_loss = train_batch(model, optimizer, loss_fn, dataloader)
    print(f"Epoch: {epoch}, train_loss: {train_loss[0]}, train_acc: {train_loss[1]}")
    scheduler.step(train_loss[0])

"""<b>Exercise 5</b>:
    
Load the weights stored in the file  "best_weights.pt" (model with the best weights).

Hint: The file contains a PyTorch's state_dict.
"""

# YOUR CODE GOES HERE

"""<b>Exercise 6:</b>
    
Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions. Create a mapping from the integers to the characters as we did by defining the dictionary char_to_int.
"""

int_to_char = {} # MODIFY

"""<b>Exercise 7:</b>

Finally, we need to actually make predictions.

The forward method of LSTM_model, has an autoregressive_iterations parameters. Use it to generate text with the LSTM.

We can pick a random input pattern as our seed sequence, then print generated characters as we generate them.

Use the pretrained (or your own trained) LSTM and predict the next 100 characters using a random sequence as starting point. How could you easily obtain different predictions for the next character? (hint torch.multinomial + torch.softmax)
"""

class LSTM_model(nn.Module):
    def __init__(self,
                 num_embeddings = 5000,
                 embedding_dim = 32,
                lstm_dim = 10,
                target_size = 1,
                recurrent_dropout = 0.2,
                seq_dropout = 0.1):
        super().__init__()
        self.lstm_dim = lstm_dim
        self.embedding = nn.Embedding(num_embeddings,
                                     embedding_dim)
        self.lstm = nn.LSTMCell(embedding_dim, lstm_dim)
        self.clf = nn.Linear(lstm_dim, target_size)
        self.recurrent_do = RecurrentDropout(p = recurrent_dropout)
        self.seq_dropout = nn.Dropout1d(p=seq_dropout)

    def forward(self, x, autoregressive_iterations=None, deterministic = True):
        x = self.embedding(x)
        x = self.seq_dropout(x)
        for i in range(x.shape[-2]):
            if i == 0:
                hn, cn = self.lstm(x[:, i])
                hn = self.recurrent_do(hn, first=True)
            else:
                hn, cn = self.lstm(x[:, i], (hn,
                                             cn))
                hn = self.recurrent_do(hn, first=False)
        if autoregressive_iterations is None:
            return self.clf(hn)
        else:
            output = []
            for it in range(autoregressive_iterations):
                if not deterministic:
                    token_ps = None
                    token = None
                else:
                    token = self.clf(hn).argmax(1)
                output += token # add token to the output
                embed_token = self.embedding(token).squeeze().unsqueeze(0)
                hn, cn = self.lstm(embed_token, (hn, cn))
        return torch.stack(output)

# Excercise
# pick a random seed
start = numpy.random.randint(0, len(dataX)-1)
pattern = dataX[start]
print("Seed:")
print(''.join([int_to_char[value] for value in pattern]))
resulting_strg = ''
pattern_tensor = torch.Tensor(pattern).long().to(device)[None, ]

model.eval()
with torch.no_grad(): # Avoid gradient computation
    # YOUR CODE GOES HERE
print("\nPrediction: " + resulting_strg)
print("\nDone.")

"""<b>Exercise 8:</b>
    
Use a sentence with at least 100 characters created by your own and look at the next predicted 100 characters. How does the prediction look like?
"""

# YOUR CODE GOES HERE