{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-I9pA7STD1w"
   },
   "source": [
    "## Part 1: LSTM for Sequence Classification on the IMDB dataset\n",
    "\n",
    "In a first exercise we want to build a model that can classify movie reviews as positive or negative based on their sentiment. To do so we train a model using the IMDB dataset.\n",
    "\n",
    "The IMDB dataset consists of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ppM5GGRvEpm5"
   },
   "outputs": [],
   "source": [
    "# # Install packages:\n",
    "# pip install torchdata\n",
    "# pip install torchtext\n",
    "\n",
    "# Additionally you may need:\n",
    "# pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RzR4apcwTD1z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "import pandas as pd\n",
    "# fix random seed so every student get the same results\n",
    "numpy.random.seed(7)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.datasets import imdb\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import portalocker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49ApijsaTD10"
   },
   "source": [
    "We need to load the IMDB dataset. We are constraining the dataset to the top 5,000 words. We also split the dataset into train (50%) and test (50%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jFqxe6EFEpm7"
   },
   "outputs": [],
   "source": [
    "class VocabOneHot(): # Helper class for processing the\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 num_words = 5000):\n",
    "        self.counts = {} # Dictionary for storing the counts of each word\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_words = num_words # maximal number of words to use\n",
    "        self.fitted = False\n",
    "        self.lookup = {} # lookup table assigning to each word an integer\n",
    "\n",
    "    def fit(self, x_iter):\n",
    "        self.counts = {} # Reinitialize the counts\n",
    "        for label, line in train_iter: #iterate over the instances\n",
    "            tokens = self.tokenizer(line)\n",
    "            for t in tokens:\n",
    "                if t in self.counts:\n",
    "                    self.counts[t] += 1\n",
    "                else:\n",
    "                    self.counts[t] = 1\n",
    "        count_series = pd.Series(self.counts).sort_values(ascending=False) # create Series for easy manipulation\n",
    "        self.lookup = {word[0]:i+1 if i<self.num_words-2 else self.num_words-1\n",
    "                       for i, word in enumerate(count_series.reset_index().to_numpy())}\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, tokenized_sentence, length = 100):\n",
    "        assert self.fitted, \"The instance has not been fitted yet\"\n",
    "        non_padded= [self.lookup[word]\n",
    "                if word in self.lookup.keys()\n",
    "                else self.num_words-1\n",
    "                    for word in tokenized_sentence]\n",
    "        non_padded = np.array(non_padded) # create unpadded version\n",
    "        if len(non_padded) == length: # leave the sequence as it stands\n",
    "            return non_padded\n",
    "        elif len(non_padded) > length: # truncate\n",
    "            return non_padded[:length]\n",
    "        else: # pad\n",
    "            padding = np.zeros(length - len(non_padded))\n",
    "            padded = np.hstack([padding, non_padded]).astype(int)\n",
    "            return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OAVOC9w1Epm7"
   },
   "outputs": [],
   "source": [
    "train_iter = imdb.IMDB(r\"data\\\\\",  split=\"train\")\n",
    "test_iter = imdb.IMDB(r\"data\\\\\", split=\"test\")\n",
    "tokenizer = get_tokenizer(\"basic_english\", language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bBow9zIJEpm8"
   },
   "outputs": [],
   "source": [
    "# Could be necessary depending of the version of torchtext\n",
    "train_iter = list(train_iter)\n",
    "test_iter = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eVJNbSh-Epm8"
   },
   "outputs": [],
   "source": [
    "vocab = VocabOneHot(tokenizer) # Initialize the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gCp4aD00Epm9"
   },
   "outputs": [],
   "source": [
    "sr = vocab.fit(train_iter) # Fit the vocabulary to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QFEXMB_DEpm_"
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "ys = []\n",
    "for i, (label, line) in enumerate(train_iter): # Create training data\n",
    "    X_train += [vocab.transform(tokenizer(line))]\n",
    "    y_train += [label]\n",
    "X_train = np.array(X_train).astype(int)\n",
    "y_train = np.array(y_train)\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i, (label, line) in enumerate(test_iter): # Create test data\n",
    "    X_test += [vocab.transform(tokenizer(line))]\n",
    "    y_test += [label]\n",
    "X_test = np.array(X_test).astype(int)\n",
    "y_test = np.array(y_test)\n",
    "y_test[y_test==1] = 0\n",
    "y_test[y_test==2] = 1\n",
    "y_train[y_train==1] = 0\n",
    "y_train[y_train==2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVjkUTBNEpm_"
   },
   "source": [
    "<b>Exercise 1</b>:\n",
    "\n",
    "Create the training data by selecting balanced subsets of the training and test splits, each of size 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9A45H6ZhEpnA"
   },
   "outputs": [],
   "source": [
    "n_pos_train_samples = np.where(y_train == 0)\n",
    "n_neg_train_samples = np.where(y_train == 1)\n",
    "\n",
    "n_pos_test_samples = np.where(y_test == 0)\n",
    "n_neg_test_samples = np.where(y_train == 1)\n",
    "\n",
    "positives_train = X_train[y_train == 0]\n",
    "negatives_train = X_train[y_train == 1]\n",
    "positives_test = X_test[y_test == 0]\n",
    "negatives_test = X_test[y_test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train[n_pos_train_samples[0][:2500]], X_train[n_neg_train_samples[0][:2500]]))\n",
    "y_train = np.concatenate((y_train[n_pos_train_samples[0][:2500]], y_train[n_neg_train_samples[0][:2500]]))\n",
    "\n",
    "X_test = np.concatenate((X_test[n_pos_train_samples[0][:2500]], X_test[n_neg_train_samples[0][:2500]]))\n",
    "y_test = np.concatenate((y_test[n_pos_train_samples[0][:2500]], y_test[n_neg_train_samples[0][:2500]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0aUWZBxWTD13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert X_train.shape == (5000, 100)\n",
    "    assert X_test.shape == (5000, 100)\n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-kBNo6XTD14"
   },
   "source": [
    "We can now define and fit our LSTM model.\n",
    "\n",
    "The first layer is the embedded layer that uses 32 length vectors to represent\n",
    " each word. The next layer is the LSTM layer with 5 memory units (smart neurons).\n",
    " Finally, because this is a classification problem we use a dense output\n",
    " layer with a single neuron and a sigmoid activation function to make 0 or 1\n",
    " predictions for the two classes (good or bad) in the problem.\n",
    "\n",
    "Because it is a binary classification problem, log loss is used as the loss\n",
    "function. The efficient ADAM optimization\n",
    "algorithm is used to do the gradient descent.\n",
    "\n",
    "<b>Exercise 2</b>:\n",
    "\n",
    "Explore the class LSTM_model, try to understand the details of the forward method, and comment the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tl_qM0A9EpnB"
   },
   "outputs": [],
   "source": [
    "# Create dataloaders containing the test and training data\n",
    "data_train = torch.utils.data.TensorDataset(torch.Tensor(X_train).long(),\n",
    "                               torch.Tensor(y_train).long())\n",
    "data_test = torch.utils.data.TensorDataset(torch.Tensor(X_test).long(),\n",
    "                               torch.Tensor(y_test).long())\n",
    "train_dl = torch.utils.data.DataLoader(data_train, batch_size = 256, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(data_test, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Hi-zelaWEpnB"
   },
   "outputs": [],
   "source": [
    "class RecurrentDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for Dropout in recurrent architectures.\n",
    "    The mask is initialized on the first iteration, and used along the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.1):\n",
    "        super().__init__()\n",
    "        self.id = nn.Identity()\n",
    "        self.bern = torch.distributions.Bernoulli(p)\n",
    "        self.p = p\n",
    "    def forward(self, x, first=True):\n",
    "        if self.training:\n",
    "            if first:\n",
    "                self.mask = self.bern.sample([x.shape[0], x.shape[1]])\n",
    "            else:\n",
    "                pass\n",
    "            x = self.id(x)\n",
    "            x[self.mask.bool()] = 0\n",
    "            return (x / (1 - self.p))\n",
    "        else:\n",
    "            return self.id(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aJiruKkPTD14"
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                num_embeddings = 5000,\n",
    "                embedding_dim = 32,\n",
    "                lstm_dim = 10,\n",
    "                target_size = 1,\n",
    "                recurrent_dropout = 0,\n",
    "                seq_dropout = 0):\n",
    "        super().__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings,\n",
    "                                     embedding_dim)\n",
    "        self.lstm = nn.LSTMCell(embedding_dim, lstm_dim)\n",
    "        self.clf = nn.Linear(lstm_dim, target_size)\n",
    "        self.recurrent_do = RecurrentDropout(p = recurrent_dropout)\n",
    "        self.seq_dropout = nn.Dropout1d(p=seq_dropout)\n",
    "\n",
    "    def forward(self, x, autoregressive_iterations=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.seq_dropout(x)\n",
    "        for i in range(x.shape[-2]):\n",
    "            if i == 0:\n",
    "                hn, cn = self.lstm(x[:, i])\n",
    "                hn = self.recurrent_do(hn, first=True)\n",
    "            else:\n",
    "                hn, cn = self.lstm(x[:, i], (hn,\n",
    "                                             cn))\n",
    "                hn = self.recurrent_do(hn, first=False)\n",
    "        if autoregressive_iterations is None:\n",
    "            return self.clf(hn)\n",
    "        else:\n",
    "            output = []\n",
    "            for it in range(autoregressive_iterations):\n",
    "                token = self.clf(hn).argmax(axis=1)\n",
    "                output += token\n",
    "                embed_token = self.embedding(token)\n",
    "                hn, cn = self.lstm(embed_token, (hn, cn))\n",
    "        return torch.stack(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_S6jVhCEpnB"
   },
   "source": [
    "Now the boilerplate code used for training and evaluating the model will be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rjfSQwzPEpnC"
   },
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, loss_fn, dataloader):\n",
    "    losses = []\n",
    "    hit = 0\n",
    "    miss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        batch[0] = batch[0].to(device)\n",
    "        batch[1] = batch[1].to(device)\n",
    "        output = model(batch[0])\n",
    "        loss = loss_fn(output.squeeze(), batch[1].squeeze().float())\n",
    "        losses += [loss.item()]\n",
    "        loss.backward()\n",
    "        pred =  (output > 0).long()\n",
    "        equal = pred.squeeze() == batch[1].squeeze()\n",
    "        hit += equal.sum()\n",
    "        miss += (~equal).sum()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(losses), hit/(hit + miss)\n",
    "\n",
    "def test_batch(model, optimizer, loss_fn, dataloader):\n",
    "    losses = []\n",
    "    hit = 0\n",
    "    miss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch[0] = batch[0].to(device)\n",
    "            batch[1] = batch[1].to(device)\n",
    "            output = model(batch[0])\n",
    "            loss = loss_fn(output.squeeze(), batch[1].squeeze().float())\n",
    "            losses += [loss.item()]\n",
    "            pred =  (output > 0).long()\n",
    "            equal = pred.squeeze() == batch[1].squeeze()\n",
    "            hit += equal.sum()\n",
    "            miss += (~equal).sum()\n",
    "    return np.mean(losses), hit/(hit + miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "liAIo-iHEpnC"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM_model()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZLDKr1-TD15"
   },
   "source": [
    "<b>Exercise 3:</b>\n",
    "\n",
    "Train a model for only 10 epochs and evaluate the model on the test data. Use a batch size of 64 reviews to update the weights in the LSTM. Use X_test and y_test to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Hj_50ZQSTD15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,\n",
      "    train_loss: 0.2755929224193096,\n",
      "    train_acc: 0.9067999720573425,\n",
      "    test_loss: 0.7459554702043534,\n",
      "    test_acc: 0.6741999983787537\n",
      "Epoch: 1,\n",
      "    train_loss: 0.2619881972670555,\n",
      "    train_acc: 0.9114000201225281,\n",
      "    test_loss: 0.7610745161771775,\n",
      "    test_acc: 0.6736000180244446\n",
      "Epoch: 2,\n",
      "    train_loss: 0.2536807142198086,\n",
      "    train_acc: 0.9157999753952026,\n",
      "    test_loss: 0.7748871296644211,\n",
      "    test_acc: 0.6790000200271606\n",
      "Epoch: 3,\n",
      "    train_loss: 0.24141697585582733,\n",
      "    train_acc: 0.920199990272522,\n",
      "    test_loss: 0.783962631225586,\n",
      "    test_acc: 0.6800000071525574\n",
      "Epoch: 4,\n",
      "    train_loss: 0.23363110944628715,\n",
      "    train_acc: 0.9241999983787537,\n",
      "    test_loss: 0.7919435352087021,\n",
      "    test_acc: 0.6837999820709229\n",
      "Epoch: 5,\n",
      "    train_loss: 0.224406498670578,\n",
      "    train_acc: 0.9283999800682068,\n",
      "    test_loss: 0.801520636677742,\n",
      "    test_acc: 0.6754000186920166\n",
      "Epoch: 6,\n",
      "    train_loss: 0.21610215678811073,\n",
      "    train_acc: 0.9300000071525574,\n",
      "    test_loss: 0.8026196956634521,\n",
      "    test_acc: 0.6772000193595886\n",
      "Epoch: 7,\n",
      "    train_loss: 0.21316298916935922,\n",
      "    train_acc: 0.9327999949455261,\n",
      "    test_loss: 0.8101830154657363,\n",
      "    test_acc: 0.676800012588501\n",
      "Epoch: 8,\n",
      "    train_loss: 0.2041427567601204,\n",
      "    train_acc: 0.9348000288009644,\n",
      "    test_loss: 0.8254348486661911,\n",
      "    test_acc: 0.6746000051498413\n",
      "Epoch: 9,\n",
      "    train_loss: 0.19628024101257324,\n",
      "    train_acc: 0.9405999779701233,\n",
      "    test_loss: 0.8521939635276794,\n",
      "    test_acc: 0.6801999807357788\n",
      "Epoch: 10,\n",
      "    train_loss: 0.19012491405010223,\n",
      "    train_acc: 0.942799985408783,\n",
      "    test_loss: 0.8570422142744064,\n",
      "    test_acc: 0.6814000010490417\n",
      "Epoch: 11,\n",
      "    train_loss: 0.18400422483682632,\n",
      "    train_acc: 0.9458000063896179,\n",
      "    test_loss: 0.8529684245586395,\n",
      "    test_acc: 0.673799991607666\n",
      "Epoch: 12,\n",
      "    train_loss: 0.17540818266570568,\n",
      "    train_acc: 0.9490000009536743,\n",
      "    test_loss: 0.8562105655670166,\n",
      "    test_acc: 0.6782000064849854\n",
      "Epoch: 13,\n",
      "    train_loss: 0.16919459067285061,\n",
      "    train_acc: 0.9517999887466431,\n",
      "    test_loss: 0.8883959919214248,\n",
      "    test_acc: 0.680400013923645\n",
      "Epoch: 14,\n",
      "    train_loss: 0.16408676393330096,\n",
      "    train_acc: 0.9527999758720398,\n",
      "    test_loss: 0.876683047413826,\n",
      "    test_acc: 0.6754000186920166\n",
      "Epoch: 15,\n",
      "    train_loss: 0.16084241196513177,\n",
      "    train_acc: 0.9544000029563904,\n",
      "    test_loss: 0.8855056524276733,\n",
      "    test_acc: 0.6725999712944031\n",
      "Epoch: 16,\n",
      "    train_loss: 0.15697767175734043,\n",
      "    train_acc: 0.9562000036239624,\n",
      "    test_loss: 0.909298625588417,\n",
      "    test_acc: 0.6776000261306763\n",
      "Epoch: 17,\n",
      "    train_loss: 0.14981746189296247,\n",
      "    train_acc: 0.9588000178337097,\n",
      "    test_loss: 0.9073829531669617,\n",
      "    test_acc: 0.6800000071525574\n",
      "Epoch: 18,\n",
      "    train_loss: 0.14805147647857667,\n",
      "    train_acc: 0.9592000246047974,\n",
      "    test_loss: 0.9317153453826904,\n",
      "    test_acc: 0.6714000105857849\n",
      "Epoch: 19,\n",
      "    train_loss: 0.14328455850481986,\n",
      "    train_acc: 0.9613999724388123,\n",
      "    test_loss: 0.9443631082773208,\n",
      "    test_acc: 0.6827999949455261\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_loss =  train_batch(model, optimizer, loss_fn, train_dl)\n",
    "    test_loss = test_batch(model, optimizer, loss_fn, test_dl)\n",
    "    print(f\"\"\"Epoch: {epoch},\n",
    "    train_loss: {train_loss[0]},\n",
    "    train_acc: {train_loss[1]},\n",
    "    test_loss: {test_loss[0]},\n",
    "    test_acc: {test_loss[1]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glEnRu1cTD15"
   },
   "source": [
    "<b>Exercise 4:</b>\n",
    "\n",
    "Once we fitted the model, we estimate the performance of the model on unseen reviews.\n",
    "\n",
    "\n",
    "Modify the test_batch function to create predictions for the test set (note that the model outputs logits and no probabilities, hint: torch.sigmoid).\n",
    "\n",
    "Predict the instances found in the test dataloader, use a ROC curve to show the performance of the model.\n",
    "\n",
    "What conclusions can you draw from the ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rbzoYFbtTD15"
   },
   "outputs": [],
   "source": [
    "def predict(model, optimizer, loss_fn, dataloader):\n",
    "   # set model to evaluate model\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = torch.tensor([], dtype=torch.int)\n",
    "    all_outputs = torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            outputs = model(batch[0])\n",
    "            y_true = torch.cat((y_true, batch[1]), 0)\n",
    "            all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "    \n",
    "    return torch.sigmoid(all_outputs), y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sN3__NsOEpnD"
   },
   "outputs": [],
   "source": [
    "y_preds, y_true = predict(model, optimizer, loss_fn, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3TN-cFRTD16"
   },
   "source": [
    "## Part 2: Generate sentences using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSxLpuZTD16"
   },
   "source": [
    "In this part of the lab we are using a pretrained model (you can train the model on your own, but this will take some time). This model was trained to predict the next character given a sequence of 100 previous characters. This model can be used to generate new sentences/phrases. Your task is to use this model and generate sentences with it. The model was pretrained on a book called \"wonderland\".\n",
    "\n",
    "First of all we want to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "klzeFZFyTD16"
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename,encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()\n",
    "# if there is an error use (python 2 vs. python 3):\n",
    "#raw_text = open(filename).read()\n",
    "#raw_text = raw_text.lower().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjvtMq63TD17"
   },
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tGn991mMTD17"
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzs7LAOATD17"
   },
   "source": [
    "For example, the list of unique unicode sorted lowercase characters in the book is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "oTuEsvmTTD17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52PQh6KuTD18"
   },
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process. In this lab we skip this process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "azb0EoHJTD18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 163817\n",
      "Total Vocab: 61\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \" + str(n_chars))\n",
    "print(\"Total Vocab: \" + str(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ggAY6HLTD18"
   },
   "source": [
    "We can see that the book has just over 160,000 characters and that when\n",
    "converted to lowercase that there are only 61 distinct characters in the vocabulary for the network to learn.\n",
    "\n",
    "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial (as explained above) we will split the book text up into subsequences with a fixed length of 100 characters.\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it.\n",
    "\n",
    "For example, if the sequence length is 5 (for simplicity) then the first two training patterns would be as follows:\n",
    "\n",
    "CHAPT -> E, HAPTE -> R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEeeBmoWTD18"
   },
   "source": [
    "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuUWk3SgTD19"
   },
   "source": [
    "<b>Exercise 5:</b>\n",
    "\n",
    "Create all patterns.\n",
    "\n",
    "Create a list of sequences dataX that contains all windows of the book and a list of following characters dataY.\n",
    "So each entry in the list dataX contains of a vector of 100 integer values representing the characters that occur in the window.\n",
    "Each entry in the list dataY contains the following character for the associated window in dataX.\n",
    "\n",
    "Hint: Use the dictionary char_to_int to map the characters to integers. You can find an example entry of dataX and dataY in the test cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CDvWAcY7TD19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 163717\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(raw_text)):\n",
    "    if i + seq_length >= len(raw_text):\n",
    "        break\n",
    "    dataX.append([char_to_int[character] for character in raw_text[i: i + seq_length]])\n",
    "    dataY.append(char_to_int[raw_text[i + seq_length]])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \" +  str(n_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiSjGMycTD19"
   },
   "source": [
    "Check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DiZFdBvFTD19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #assert n_patterns >= 163717\n",
    "    assert dataX[0] == [60, 45, 47, 44, 39, 34, 32, 49, 1, 36, 50, 49, 34, 43, 31, 34, 47, 36, 57, 48, 1, 30, 41, 38, 32, 34, 57, 48, 1, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 1, 38, 43, 1, 52, 44, 43, 33, 34, 47, 41, 30, 43, 33, 9, 1, 31, 54, 1, 41, 34, 52, 38, 48, 1, 32, 30, 47, 47, 44, 41, 41, 0, 0, 49, 37, 38, 48, 1, 34, 31, 44, 44, 40, 1, 38, 48, 1, 35, 44, 47, 1, 49, 37, 34, 1, 50, 48, 34, 1, 44]\n",
    "    assert dataY[0] == 35\n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcppxYuyTD19"
   },
   "source": [
    "Running the code to this point shows us that when we split up the dataset into training data for the network to learn that we have just under 160,000 training patterns.\n",
    "\n",
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, features] expected by an LSTM network (if we would have more features per time step the dimension would be [samples, time_steps, features]).\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 61 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 61, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wZBrYp4-TD19"
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length))\n",
    "# one hot encode the output variable\n",
    "Y_onehot = np.eye(len(chars))[dataY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf1MV0-hTD1-"
   },
   "source": [
    "Now we can define our model. It consists of an embedding layer that embeds each character in a 32-dimensional feature space. The next layer is a layer with 256 LSTM units followed by a Dropout-Layer to reduce overfitting. The last layer is a Dense-Layer used to predict the probabilities for each of the 61 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7civDBPETD1-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_model(\n",
       "  (embedding): Embedding(61, 32)\n",
       "  (lstm): LSTMCell(32, 256)\n",
       "  (clf): Linear(in_features=256, out_features=61, bias=True)\n",
       "  (recurrent_do): RecurrentDropout(\n",
       "    (id): Identity()\n",
       "  )\n",
       "  (seq_dropout): Dropout1d(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_vector_length = 32\n",
    "top_words = n_vocab\n",
    "model = LSTM_model(num_embeddings = 61,\n",
    "                 embedding_dim = 32,\n",
    "                lstm_dim = 256,\n",
    "                target_size = 61,\n",
    "                recurrent_dropout=0.15,\n",
    "                seq_dropout=0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwEQwDnpTD1-"
   },
   "source": [
    "We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "G383P94cEpnH"
   },
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, loss_fn, dataloader):\n",
    "    losses = []\n",
    "    hit = 0\n",
    "    miss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        batch[0] = batch[0].to(device)\n",
    "        batch[1] = batch[1].to(device)\n",
    "        output = model(batch[0])\n",
    "        loss = loss_fn(output.squeeze(), batch[1].squeeze().float())\n",
    "\n",
    "        losses += [loss.item()]\n",
    "        loss.backward()\n",
    "        pred =  output.argmax(axis=1)\n",
    "        equal = pred.squeeze() == batch[1].argmax(axis=1).squeeze()\n",
    "        hit += equal.sum()\n",
    "        miss += (~equal).sum()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(losses), hit/(hit + miss)\n",
    "def test_batch(model, optimizer, loss_fn, dataloader):\n",
    "    losses = []\n",
    "    hit = 0\n",
    "    miss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch[0] = batch[0].to(device)\n",
    "            batch[1] = batch[1].to(device)\n",
    "            output = model(batch[0])\n",
    "            loss = loss_fn(output.squeeze(), batch[1].argmax(axis=1).squeeze().float())\n",
    "            losses += [loss.item()]\n",
    "            pred =  output.argmax(axis=1)\n",
    "            equal = pred.squeeze() == batch[1].squeeze()\n",
    "            hit += equal.sum()\n",
    "            miss += (~equal).sum()\n",
    "    return np.mean(losses), hit/(hit + miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "1nplD_GBTD1-"
   },
   "outputs": [],
   "source": [
    "data = torch.utils.data.TensorDataset(torch.Tensor(dataX).long(),\n",
    "                                        torch.Tensor(Y_onehot).long())\n",
    "dataloader = torch.utils.data.DataLoader(data, batch_size = 256, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA8FiANUTD1_"
   },
   "source": [
    "Training this model really takes some time. So you can skip the learning step and use the pretrained model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "5yLcvZPRTD1_"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m20\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, train_loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, train_acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep(train_loss[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[1;32mIn[45], line 13\u001B[0m, in \u001B[0;36mtrain_batch\u001B[1;34m(model, optimizer, loss_fn, dataloader)\u001B[0m\n\u001B[0;32m     10\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(output\u001B[38;5;241m.\u001B[39msqueeze(), batch[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m     12\u001B[0m losses \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [loss\u001B[38;5;241m.\u001B[39mitem()]\n\u001B[1;32m---> 13\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m pred \u001B[38;5;241m=\u001B[39m  output\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     15\u001B[0m equal \u001B[38;5;241m=\u001B[39m pred\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;241m==\u001B[39m batch[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\UNI-Machine-Learning-II\\venv\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\UNI-Machine-Learning-II\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_loss = train_batch(model, optimizer, loss_fn, dataloader)\n",
    "    print(f\"Epoch: {epoch}, train_loss: {train_loss[0]}, train_acc: {train_loss[1]}\")\n",
    "    scheduler.step(train_loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzIZ16OGTD1_"
   },
   "source": [
    "<b>Exercise 5</b>:\n",
    "    \n",
    "Load the weights stored in the file  \"best_weights.pt\" (model with the best weights).\n",
    "\n",
    "Hint: The file contains a PyTorch's state_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4FOCjTpiTD1_"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTM_model:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([61, 32]) from checkpoint, the shape in current model is torch.Size([5000, 32]).\n\tsize mismatch for lstm.weight_ih: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([40, 32]).\n\tsize mismatch for lstm.weight_hh: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([40, 10]).\n\tsize mismatch for lstm.bias_ih: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([40]).\n\tsize mismatch for lstm.bias_hh: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([40]).\n\tsize mismatch for clf.weight: copying a param with shape torch.Size([61, 256]) from checkpoint, the shape in current model is torch.Size([1, 10]).\n\tsize mismatch for clf.bias: copying a param with shape torch.Size([61]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [38], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m path: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Users/I518095/Documents/GitHub/UNI-Machine-Learning-II/Lab 3/best_weights.pt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2152\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2147\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2148\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2149\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2152\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2153\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for LSTM_model:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([61, 32]) from checkpoint, the shape in current model is torch.Size([5000, 32]).\n\tsize mismatch for lstm.weight_ih: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([40, 32]).\n\tsize mismatch for lstm.weight_hh: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([40, 10]).\n\tsize mismatch for lstm.bias_ih: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([40]).\n\tsize mismatch for lstm.bias_hh: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([40]).\n\tsize mismatch for clf.weight: copying a param with shape torch.Size([61, 256]) from checkpoint, the shape in current model is torch.Size([1, 10]).\n\tsize mismatch for clf.bias: copying a param with shape torch.Size([61]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "path: str = r'/Users/I518095/Documents/GitHub/UNI-Machine-Learning-II/Lab 3/best_weights.pt'\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFmrtx2ITD1_"
   },
   "source": [
    "<b>Exercise 6:</b>\n",
    "    \n",
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions. Create a mapping from the integers to the characters as we did by defining the dictionary char_to_int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "AisoE-r_TD1_"
   },
   "outputs": [],
   "source": [
    "int_to_char = {item: key for key, item in char_to_int.items()} # MODIFY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKq89CxDTD2A"
   },
   "source": [
    "<b>Exercise 7:</b>\n",
    "\n",
    "Finally, we need to actually make predictions.\n",
    "\n",
    "The forward method of LSTM_model, has an autoregressive_iterations parameters. Use it to generate text with the LSTM.\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtlchGz9TD2A"
   },
   "source": [
    "Use the pretrained (or your own trained) LSTM and predict the next 100 characters using a random sequence as starting point. How could you easily obtain different predictions for the next character? (hint torch.multinomial + torch.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "MsJ0MPiOEpnK"
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_embeddings = 5000,\n",
    "                 embedding_dim = 32,\n",
    "                lstm_dim = 10,\n",
    "                target_size = 1,\n",
    "                recurrent_dropout = 0.2,\n",
    "                seq_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings,\n",
    "                                     embedding_dim)\n",
    "        self.lstm = nn.LSTMCell(embedding_dim, lstm_dim)\n",
    "        self.clf = nn.Linear(lstm_dim, target_size)\n",
    "        self.recurrent_do = RecurrentDropout(p = recurrent_dropout)\n",
    "        self.seq_dropout = nn.Dropout1d(p=seq_dropout)\n",
    "\n",
    "    def forward(self, x, autoregressive_iterations=None, deterministic = True):\n",
    "        x = self.embedding(x)\n",
    "        x = self.seq_dropout(x)\n",
    "        for i in range(x.shape[-2]):\n",
    "            if i == 0:\n",
    "                hn, cn = self.lstm(x[:, i])\n",
    "                hn = self.recurrent_do(hn, first=True)\n",
    "            else:\n",
    "                hn, cn = self.lstm(x[:, i], (hn,\n",
    "                                             cn))\n",
    "                hn = self.recurrent_do(hn, first=False)\n",
    "        if autoregressive_iterations is None:\n",
    "            return self.clf(hn)\n",
    "        else:\n",
    "            output = []\n",
    "            for it in range(autoregressive_iterations):\n",
    "                if not deterministic:\n",
    "                    token_ps = None\n",
    "                    token = None\n",
    "                else:\n",
    "                    token = self.clf(hn).argmax(1)\n",
    "                output += token # add token to the output\n",
    "                embed_token = self.embedding(token).squeeze().unsqueeze(0)\n",
    "                hn, cn = self.lstm(embed_token, (hn, cn))\n",
    "        return torch.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "TYIx9aGaTD2A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "they couldn’t get\n",
      "them out again. that’s all.’\n",
      "\n",
      "‘thank you,’ said alice, ‘it’s very interesting. i n\n",
      "\n",
      "Prediction: ever see it was a little birds and the mouse to herself, ‘i don’t know the work of the sea, the mous\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Excercise\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(''.join([int_to_char[value] for value in pattern]))\n",
    "resulting_strg = ''\n",
    "pattern_tensor = torch.Tensor(pattern).long().to(device)[None, ]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # Avoid gradient computation\n",
    "    output = model.forward(pattern_tensor, autoregressive_iterations=100)\n",
    "\n",
    "for character in output.tolist():\n",
    "    resulting_strg += int_to_char[character]\n",
    "    \n",
    "print(\"\\nPrediction: \" + resulting_strg)\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DrwfJuzTD2A"
   },
   "source": [
    "<b>Exercise 8:</b>\n",
    "    \n",
    "Use a sentence with at least 100 characters created by your own and look at the next predicted 100 characters. How does the prediction look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "4JzEX68uEpnL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction: ever see it was a little birds and the mouse to herself, ‘i don’t know the work of the sea, the mous\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "my_text = 'Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut l'.lower()\n",
    "my_text_embedd = [char_to_int[c] for c in my_text]\n",
    "my_pattern_tensor = torch.Tensor(my_text_embedd).long().to(device)[None, ]\n",
    "resulting_strg = ''\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # Avoid gradient computation\n",
    "    output = model.forward(my_pattern_tensor, autoregressive_iterations=100)\n",
    "\n",
    "for character in output.tolist():\n",
    "    resulting_strg += int_to_char[character]\n",
    "    \n",
    "print(\"\\nPrediction: \" + resulting_strg)\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
